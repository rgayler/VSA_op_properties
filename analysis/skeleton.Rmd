---
title: "Skeleton of paper"
author: Charles T. Gray, Ross W. Gayler, Stefan Reimann
date: "2020-06-10"
output: workflowr::wflow_html
bibliography: VSA_operators.bibtex
csl: apa-single-spaced.csl
editor_options:
  chunk_output_type: console
---

This is the skeleton of the output paper.
It also contains some meta-level notes.

Notation: **&rarr;XX** indicates a question or action aimed at person *XX*.

Ideally there would be some way of cross-referencing points in the `skeleton` document
with code chunks and text in the `simulation` document
but this doesn't seem to be possible when using `{workflowr}`.
I don't want to merge the two documents into one
because I think that would be very unwieldy
while trying to reorganise the skeleton.

---

RG expects that this will be quite a long paper
because of the amount of relevant background to be covered
(the empirical component will be relatively straightforward).
It **won't** be suitable for a classic computer science 6-page conference paper.
FWIW, [PeerJ Computer Science](https://peerj.com/computer-science/)
doesn't appear to have a page limit.

**&rarr;SR** A lot of the background information I expect to need here
should probably be included in SR's cookbook.
If that is so, would it be better to treat the cookbook as the primary repository for that information
and have pointers from this paper?
That would make this paper shorter, but no longer self-contained.
That would also raise issues about the publication order of this paper and the cookbook,
and how to deal with changing content if the cookbook evolves over time.

**&rarr;CG** - For everything that is done empirically
it would be helpful to have mathematical argument showing what results are expected.
To the extent that is possible you could, in principle,
run with the maths and drop the empirical component -
but RG likes empirical demonstration
to give some confidence that the maths isn't flawed.

# Overview

VSA is about using mathematical operators on very high dimensional vector sppaces as a basis for computation.
They are interesting because they are very robust to noise and implementation errors
which suits them to newly emerging computer technologies which are inherently less reliable
as a consequence of their extreme miniaturisation.
They are also of interest because of their potential relevance to neural computation.

VSAs are a family of related approaches.
Different researchers have devised a variety of instantiations of VSAs, 
indicating that the primary computatuional properties
are not strongly dependent on the details that vary between instantiations.
These instantiations have been compared and contrasted,
however in this work we are approaching from the other direction
and exploring what mathematical properties are essential to VSAs.

VSAs are relatively simple systems
involving a vector space and a small number of mathematical operators on the vector space.
We explore conceptually, mathematically, and empirically the mathematical properties of these entities
that are needed to make VSAs computationally useful.

As an example, Stefan Reimann introduced a non-associative variant of the sum operator.
The non-associativity is useful to capture some behavioural asymmetries in the modelling of human memory.
Stefan used a probabilistic parameterisation of an extensive tabular definition of the operator.
This can probably be expanded to cover the two standard VSA binary operators.
Then the algebraic properties of the operators (e.g commutativity, associativity)
can be mapped as a function of location in the parameter space.

This is exploratory work, so the exploration may demonstrate that the above path is untenable,
in which case we would need to distill from the exploration the lessons we think most useful for others.

# Introduction

* Something like the introduction of @schlegelComparisonVectorSymbolic2020

* Note the focus on implementation (biological or artificial).
**RG says this focus on implementation is a defining feature of VSA.**
This may influence some of the mathematical choices made later.
Emphasise that the maths is a *model* of implementable systems rather than free-standing maths.
  * RG vaguely remembers recently seeing a mention of graph-structured computation
  (i.e. data flow graphs rather than graphs as the subject of computation)
  as a known area of study.  
  **&rarr;RG** Find references to computation graphs.  
  **&rarr;CG?** Check whether computation graphs is of any use to us.
  * This point about focus on implementation 
  is most likely relevant in that parenthesization of math expressions to represent associativity
  translates into alternate wiring of the implementation data flow graph.
    * **&rarr;RG** Give example here with diagrams.
    
As a test of between-file cross-referencing [see](simulation.html#data-flow-graph)

As a test of within-file figure cross-referencing see \@ref(fig:dfg-sum-ab)
  
* A data flow graph is actually an oriented *hypergraph*,
where hyperedges correspond to operators/functions with distinguished arguments and output,
and vertices correspond to HD vector values (thought of as 'registers' if you like).
  * **&rarr;CG** Side question: Vector space maths seems to be written as though there's only one vector space.
  In an implementation there are (arguably) as many vector spaces as there are vertices in the data flow hypergraph.
  Is this a relevant distinction and worth pointing out?
    * For example, we generally assume that each of the vertices has the same dimensionality,
    but that's not essential, and in the biological setting it's unlikely.
    * Along the same lines, permutation operators feel completely different in a biological setting:
    The input and output vertices are generally not identical dimensionality;
    The neurons aren't nicely indexed, that's externally applied labelling.
    Given that arbitrariness, every connection between two registers is a permutation.
    * Also, in mathematical and artificial systems we can easily have permutations and their inverses,
    whereas in biological systems inverse permutations are implausible.

# Content

## Basic set of VSA operators

* Park discussion of the more unusual operators like CDT binding until we have an agreed structure for the typical operators.

* Sum-like, product-like, permutation
  * Commonly thought of as binding, bundling, protect because that's how they are typically used in systems
  but best not to think of them that way.
  The operators have mathematical properties
  and the job of the system designer is to design the system to extract useful work from those properties.

* The operators are binary, binary, unary (respectively) on a vector space.
  * For initial explanation, think of a high-dimensional (say, 10k) real vector space

* **&rarr;CG** Side question: Does maths unreasonably privilege unary and binary operators over operators with more arguments
(possibly because a written representation only offers before and after positions for the arguments wrt an infix operator)
or are unary and binary operators fundamental in some sense?

* Sum-like operator is *element-wise* 'sum' of vectors

* Permutation operator is a fixed re-ordering of elements in a vector.
  * In a fixed finite data flow hypergraph (i.e. a physical realisation)
  there would be a fixed, finite number of specific permutations.
  * In an artificial system the same permutation could appear at multiple locations in the data flow hypergraph,
  and having one permutation be the inverse of another is no problem.
  * In a biological system each permutation would most likely be unique,
  and getting an inverse to some other permutation is unlikely.
  
* Product-like operator is a bit more complex
* Start with @smolenskyTensorProductVariable1990: tensor product variable binding 
  * Showed that it is possible to represent composite discrete data structures in a vector space
  and manipulate those structures with vector space transformations.
  * Product-like operator is tensor product (= outer product)
    * Very inconvenient for implementation because dimensionality increases exponentially with the number vectors combined
  * Mention Fock algebras? (e.g. @beimgrabenVectorSymbolicArchitectures2020, although I think there's a much earlier mention by Smolensky)
  * The elements of a tensor are structured wrt the indices,
  but it is possible to treat a tensor as a vector by discarding the index structure of the elements.
* VSA can be interpreted as a lossy compression of the outer product (Plate 1994 - Thesis?)
  * Calculate tensor product and project onto a lower dimensional space (same dimensionality as the arguments)
  * Plate's HRR uses circular convolution as the product operator
    * Circular convolution is defined as a very orderly set of sums over the elements of the outer product
      * Include Plate's standard diagram of circular convolution
  * Plate pointed out that sums of random subsets of the elements of the outer product would also work
  * Relate to Johnson-Lindenstrauss Lemma (@venkatasubramanianJohnsonLindenstraussTransformEmpirical2011)?
  (Is this a good reference?)
    * This might make more sense after covering the centrality of vector similarity in VSA
    
* RG would be very interested to see an empirical demonstration of the similarity of tensor representations interpreted as vectors
  * Also project the vector representations to higher and lower dimensionality.

## Vector space

* Different bases (terminology)
  * complex, real, {0,1}, {-1,+1}, whatever the hell geometric algebras are
* Dimensionality compensates for resolution @plateDistributedRepresentationsNested1994, pages ???
* Is it arguable to run with binary resolution as our guinea-pig?
  * @plateDistributedRepresentationsNested1994 pages ??? made some points about element resolution
  * ... also pointed out that BSC is equivalent to Frequency domain HRR with phase quantised to binary levels

* Interpretation of vectors as analog representation of discrete structure (cite RG talk at Redwood Center?)
  * Direction encodes sequence of operators and arguments applied
  * Magnitude encodes 'support' for the representation

## Similarity {#sim}

* Similarity measure is core to VSA
  * Argue that point from a VSA pragmatics view
  * **&rarr;CG** Is the similarity measure (inner product?) an essential part of the definition of a vector space?
* Argument for angular measure of similarity (distance doesn't work in HD)
* Similarity measure as sum over element-wise similarity
  * Makes element similarity definition important
  * Makes vector similarity inherently graded (because its the average over a large number of elements)
    * even if the element similarity is not graded

* Is it arguable that graded vector similarity is essential for VSA?
  * Allows weighted sums of vectors
  * Allows graded evolution of solution vectors in recurrent circuits
    * Accumulation of weak evidence

* Raise centring of similarity measure as an issue.
  * Probably do empirical work using both centred and non-centred similarity.
  * Relationship to things like non-negative matrix factorisation?
  * Domain-specific utility? e.g. existence/probability is non-negative, whereas weight of evidence is signed
  
## Definition of operators in terms of similarity effects

* Sum-like: 
  * sum similar to both arguments

* Product-like: 
  * product dissimilar to both arguments
  * product preserves similarity structure between vectors

* Permutation: 
  * permutation dissimilar to argument
  * permutation preserves similarity structure between vectors
  
* Needs some thought on the relationship between product and permutation
* **&rarr;CG?** Category theorists have views on the essence of sum-like and product-like operators?

## Properties of full-outer-product compressions

## Properties of diagonal-outer-product compressions

* Diagonal outer-product compressions are element-wise
  * Do all algebraic properties follow from considering only one element? (CG?)
  
### Probabilistic parameterisation of single-element binary operator {#prob-param}

* Focus on implementation means that operator definition must be 'local'
(i.e. output must depend only on the inputs and locally available information)
  * 2 inputs, 2 constants (the binary levels), dynamically realised random values (for probabilistic selection)
  * Implement operator table as probabilistic selection from:
      * inputs
      * constants
  * Express binary levels as {A, not_A} rather than {0, 1} or {-1, +1}
  to avoid being specific to a particular encoding?
  
* Probabilistic selection of constants raises the issue of density and density-preserving operations.
  * draw the parallel to normalisation in HRR and similar VSAs?

### Map of operator properties

* Commutativity and associativity
  * **&rarr;CG** Is that all? Are there other algebraic properties that are fundamental?
* Distributivity requires two operators
* VSA distributes wrt product and permutation
* Distributivity is important implementation property because it gives parallelism for free

# References

* The papers currently in the Zotero group library are probable references.
  * RG may add some more structure to the group library if needed.
  * RG will sort out some process to export the used references to the project directory.

* Include software attributions, especially {workflowr}

  > Workflowr is available under the MIT license. For proper attribution, please cite our manuscript that describes the software:
  >
  > Blischak JD, Carbonetto P, and Stephens M. Creating and sharing reproducible research code the workflowr way [version 1; peer review: 3 approved]. F1000Research 2019, 8:1749 (https://doi.org/10.12688/f1000research.20843.1)
  >
  > To obtain a BibTeX entry, please run citation("workflowr"). Note that F1000Research publishes not only the original version but also any revisions. To check for the latest version, please go to the paper's URL.

---



---
title: "Skeleton of paper"
author: Charles T. Gray, Ross W. Gayler, Stefan Reimann
date: "2020-06-10"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

This is the skeleton of the output paper.
It also contains some meta-level notes.

---

RG expects that this will be quite a long paper
because of the amount of relevant background to be covered
(the empirical component will be relatively straightforward).
It **won't** be suitable for a classic computer science 6-page conference paper.
FWIW, [PeerJ Computer Science](https://peerj.com/computer-science/)
doesn't appear to have a page limit.

CG - For everything that is done empirically
it would be helpful to have mathematical argument showing what is expected.
To the extent that is possible you could, in principle,
run with the maths and drop the empirical component -
but I like empirical demonstration
to give some confidence that the maths isn't flawed.

# Introduction

* Something like the introduction of [Schlegel et al 2020](https://arxiv.org/abs/2001.11797)

* Note the focus on implementation (biological or artificial).
This may influence some of the mathematical choices made later.
Emphasise that the maths is a *model* of implementable systems rather than free-standing maths.
  * RG vaguely remembers recently seeing a mention of graph-structured computation
  (i.e. data flow graphs rather than graphs as the subject of computation)
  as a known area of study.
  Check whether this area is of any use to us (CG?).
  * This point about focus on implementation 
  is most likely relevant in that parenthesization of math expressions to represent associativity
  translates into alternate wiring of the implementation data flow graph.
  
* A data flow graph is probably actually an oriented *hypergraph*,
where hyperedges correspond to operators/functions with distinguished arguments and output,
and vertices correspond to HD vector values (thought of as 'registers' if you like).
  * Side question to CG: Vector space maths seems to be written as though there's only one vector space.
  In an implementation there are (arguably) as many vector spaces as there are vertices in the data flow hypergraph.
  Is this a relevant distinction and worth pointing out?
    * For example, we generally assume that each of the vertices has the same dimensionality,
    but that's not essential, and in the biological setting it's unlikely.
    * Along the same lines, permutation operators feel completely different in a biological setting:
    The input and output vertices are generally not identical dimensionality;
    The neurons aren't nicely indexed, that's externally applied labelling; 
    Given that arbitrariness, every connection between two registers is a permutation.
    * Also, in mathematical and artificial systems we can easily have permutations and their inverses,
    whereas in biological systems inverse permutations are implausible.

# Content

## Basic set of VSA operators

* At this point, ignore the more unusual operators like CDT binding.

* Sum-like, product-like, permutation
  * Commonly thought of as binding, bundling, protect because that's how they are typically used in systems
  but best not to think of them that way.
  The operators have mathematical properties
  and the job of the system designer is to design the system to extract useful work from those properties.

* The operators are binary, binary, unary (respectively) on a vector space.
  * For initial explanation, think of a high-dimensional (say, 10k) real vector space
  
* Sum-like operator is *element-wise* 'sum' of vectors

* Permutation operator is a fixed re-ordering of elements in a vector.
  * In a fixed finite data flow hypergraph (i.e. a physical realisation)
  there would be a fixed, finite number of permutations.
  * In an artificial system the same permutation could appear at multiple locations in the data flow hypergraph,
  and having one permutation be the inverse of another is no problem.
  * In a biological system each permutation would most likely be unique,
  and getting an inverse to some other permutation is unlikely.
  
* Product-like operator is a bit more complex
* Start with Smolensky 1990 - Tensor product variable binding 
  * Showed that it is possible to represent composite discrete data structures in a vector space
  and manipulate those structures with vector space transformations.
  * Product-like operator is tensor product (= outer product)
    * Very inconvenient for implementation because dimensionality increases exponentially with the number vectors combined
  * Mention Fock algebras?
* VSA can be interpreted as a lossy compression of the outer product (Plate 1994 - Thesis?)
  * Calculate tensor product and project onto a lower dimensional space (same dimensionality as the arguments)
  * Plate's HRR uses circular convolution as the product operator
    * Circular convolution is defined as a very orderly set of sums over the elements of the outer product
      * Include Plate's standard diagram of circular convolution
  * Plate pointed out that sums of random subsets of the elements of the outer product would also work
  * Relate to Johnson-Lindenstrauss theorem?
  This would make more sense after covering the centrality of vector similarity

## Vector space

* Different bases (terminology)
* Dimensionality compensates for resolution
* Arguable to run with binary resolution as our guinea-pig?
  * Plate 1994 had some points about resolution
  * ... also pointed out that BSC is equivalent to Frequency domain HRR with phase uantised to binary levels

* Interpretation of vectors as analog representation of discrete structure
  * Direction encodes sequence of operators and arguments applied
  * Magnitude encodes 'support' for the representation

## Similarity

* Similarity measure is core (math argument for that?)
* Argument for angular measure if similarity (distance doesn't work in HD)
* Similarity measure as sum over elements

* Need for graded similarity?

* Raise centring of similarity measure as an issue.
  * Probably do empirical work using both centred and non-centred similarity.
  
## Definition of operators in terms of similarity effects

* Sum-like: 
  * sum similar to both arguments

* Product-like: 
  * product dissimilar to both arguments
  * product preserves similarity structure between vectors

* Permutation: 
  * permutation dissimilar to argument
  * permutation preserves similarity structure between vectors
  
* Needs some thought on the relationship between product and permutation
* Category theorists have views on the essence of sum-like and product-like operators? (CG?)

## Properties of full-outer-product compressions

## Properties of diagonal-outer-product compressions

* Diagonal outer-product compressions are element-wise
  * Do all algebraic properties follow from considering only one element? (CG?)
  
### Probabilistic parameterisation of single-element binary opertor

* Focus on implementation means operator definition must be 'local'
(i.e. output must depend only on the inputs and locally available information)
  * 2 inputs, 2 constants (the binary levels), dynamically realised random values (for probabilistic selection)
  * Implement operator table as probabilistic selection from:
      * inputs
      * constants
  * Express binary levels as {A, not_A} rather than {0, 1} or {-1, +1}
  to avoid being specific to a particular encoding?
  
* Probabilistic selection of constants raises the issue of density and density-preserving operations.
  * draw the parallel to normalisation in HRR and similar VSAs?

### Map of operator properties

* Commutativity and associativity (CG is that all?)
* Distributivity requires two operators
* VSA distributes wrt product and permutation
* Distributivity is important implementation property because it gives parallelism for free

# References

* The papers currently in the Zotero group library are probable references.
  * I may add some more structure to the group library if needed.
  * I will sort out some process to export the used references to the project directory.

* Include software attributions, especially {workflowr}

  > Workflowr is available under the MIT license. For proper attribution, please cite our manuscript that describes the software:
  >
  > Blischak JD, Carbonetto P, and Stephens M. Creating and sharing reproducible research code the workflowr way [version 1; peer review: 3 approved]. F1000Research 2019, 8:1749 (https://doi.org/10.12688/f1000research.20843.1)
  >
  > To obtain a BibTeX entry, please run citation("workflowr"). Note that F1000Research publishes not only the original version but also any revisions. To check for the latest version, please go to the paper's URL.



@article{beimgrabenVectorSymbolicArchitectures2020,
  title = {Vector Symbolic Architectures for Context-Free Grammars},
  author = {{beim Graben}, Peter and Huber, Markus and Meyer, Werner and R{\"o}mer, Ronald and Tsch{\"o}pe, Constanze and Wolff, Matthias},
  year = {2020},
  month = mar,
  abstract = {Background / introduction. Vector symbolic architectures (VSA) are a viable approach for the hyperdimensional representation of symbolic data, such as documents, syntactic structures, or semantic frames. Methods. We present a rigorous mathematical framework for the representation of phrase structure trees and parse-trees of context-free grammars (CFG) in Fock space, i.e. infinite-dimensional Hilbert space as being used in quantum field theory. We define a novel normal form for CFG by means of term algebras. Using a recently developed software toolbox, called FockBox, we construct Fock space representations for the trees built up by a CFG left-corner (LC) parser. Results. We prove a universal representation theorem for CFG term algebras in Fock space and illustrate our findings through a low-dimensional principal component projection of the LC parser states. Conclusions. Our approach could leverage the development of VSA for explainable artificial intelligence (XAI) by means of hyperdimensional deep neural computation. It could be of significance for the improvement of cognitive user interfaces and other applications of VSA in machine learning.},
  archivePrefix = {arXiv},
  eprint = {2003.05171},
  eprinttype = {arxiv},
  file = {/home/ross/Zotero/storage/CLJN5WSN/beim Graben et al_2020_Vector symbolic architectures for context-free grammars.pdf;/home/ross/Zotero/storage/QSQUF98M/2003.html},
  journal = {arXiv:2003.05171 [cs, q-bio]},
  keywords = {Computer Science - Computation and Language,Quantitative Biology - Neurons and Cognition},
  primaryClass = {cs, q-bio}
}

@article{borsellinoConvolutionCorrelationAlgebras1973,
  title = {Convolution and Correlation Algebras},
  author = {Borsellino, A. and Poggio, T.},
  year = {1973},
  month = sep,
  volume = {13},
  pages = {113--122},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00288790},
  abstract = {An algebraic characterization of convolution and correlation is outlined. The basic algebraic structures generated on a suitable vector space by the two operations are described. The convolution induces an associative Abelian algebra over the real field; the correlation induces a not-associative, not-commutative \textemdash{} but Lieadmissible algebra \textemdash{} with a left unity. The algebraic connection between the two algebras is found to coincide with the relation of isotopy, an extension of the concept of equivalence. The interest of these algebraic structures with respect to information processing is discussed.},
  file = {/home/ross/Zotero/storage/5CMM3V6Z/Borsellino_Poggio_1973_Convolution and correlation algebras.pdf},
  journal = {Kybernetik},
  language = {en},
  number = {2}
}

@article{kanervaHyperdimensionalComputingIntroduction2009,
  title = {Hyperdimensional {{Computing}}: {{An Introduction}} to {{Computing}} in {{Distributed Representation}} with {{High}}-{{Dimensional Random Vectors}}},
  author = {Kanerva, Pentti},
  year = {2009},
  month = jan,
  volume = {1},
  pages = {139--159},
  issn = {1866-9956},
  doi = {10.1007/s12559-009-9009-8},
  abstract = {The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness. They include Holographic Reduced Repre- sentations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector- Symbolic Architecture. They represent things in high- dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of tradi- tional computing, in what is called here hyperdimensional computing on account of the very high dimensionality. The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative. A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end. The thesis of the paper is that hyperdi- mensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.},
  file = {/home/ross/Zotero/storage/N2JWFJDX/Kanerva - 2009 - Hyperdimensional Computing An Introduction to Computing in Distributed Representation with High-Dimensional Random V(2).pdf;/home/ross/Zotero/storage/T2PUIQKM/Kanerva - 2009 - Hyperdimensional Computing An Introduction to Computing in Distributed Representation with High-Dimensional Random Vect.pdf},
  journal = {Cognitive Computation},
  keywords = {Cognitive code,cogsci,Holistic mapping,Holistic record,Holographic reduced representation,Random indexing,von Neumann architecture},
  number = {2}
}

@article{patyk-lonskaComparisonGeometricAnalogues2011,
  title = {A Comparison of Geometric Analogues of Holographic Reduced Representations, Original Holographic Reduced Representations and Binary Spatter Codes},
  author = {{Patyk-Lonska}, Agnieszka and Czachor, Marek and Aerts, Diederik},
  year = {2011},
  pages = {221--228},
  abstract = {Geometric Analogues of Holographic Reduced Representations (GA HRR) employ role-filler binding based on geometric products. Atomic objects are real-valued vectors in n-dimensional Euclidean space and complex statements belong to a hierarchy of multivectors. The paper reports a battery of tests aimed at comparison of GA HRR with Holographic Reduced Representation (HRR) and Binary Spatter Codes (BSC). Firstly, we perform a test of GA HRR which is analogous to the one proposed by Plate in [13]. Plate's simulation involved several thousand 512-dimensional vectors stored in clean-up memory. The purpose was to study efficiency of HRR but also to provide a counterexample to claims that role-filler representations do not permit one component of a relation to be retrieved given the others. We repeat Plate's test on a continuous version of GA HRR \&\#x2014; GA{$<$}inf{$>$}c{$<$}/inf{$>$} (as opposed to its discrete version described in [12]) and compare the results with the original HRR and BSC. The object of the test is to construct statements concerning multiplication and addition. For example, \&\#x201C;2\&\#x00B7;3 \&\#x003D; 6\&\#x201D; is constructed as times{$<$}inf{$>$}2,3{$<$}/inf{$>$} \&\#x003D; times\&\#x002B;operand\&\#x2217;(num{$<$}inf{$>$}2{$<$}/inf{$>$} \&\#x002B; num{$<$}inf{$>$}3{$<$}/inf{$>$})\&\#x002B;result\&\#x2217;num{$<$}inf{$>$}6{$<$}/inf{$>$}. To look up this vector one then constructs a similar statement with one of the components missing and checks whether it points correctly to times{$<$}inf{$>$}2,3{$<$}/inf{$>$}. We concentrate on comparison of recognition percentage for the three models for comparable data size, rather than on the time taken to achieve high percentage. Results show that the best models for storing and recognizing multiple similar statements are GA{$<$}inf{$>$}c{$<$}/inf{$>$} and Binary Spatter Codes with recognition percentage highly above 90.},
  file = {/home/ross/Zotero/storage/TBWVYCYV/Patyk-Lonska et al_2011_A comparison of geometric analogues of holographic reduced representations,.pdf},
  isbn = {978-83-60810-35-4},
  journal = {2011 Federated Conference on Computer Science and Information Systems (FedCSIS)},
  keywords = {BSC,distributed representations,geometric algebra,HRR,scaling}
}

@techreport{patyk-lonskaGeometricAlgebraModel2010,
  title = {Geometric {{Algebra Model}} of {{Distributed Representations}}},
  author = {{Patyk-Lonska}, Agnieszka},
  year = {2010},
  month = mar,
  pages = {30},
  abstract = {Formalism based on GA is an alternative to distributed representation models developed so far --- Smolensky's tensor product, Holographic Reduced Representations (HRR) and Binary Spatter Code (BSC). Convolutions are replaced by geometric products, interpretable in terms of geometry which seems to be the most natural language for visualization of higher concepts. This paper recalls the main ideas behind the GA model and investigates recognition test results using both inner product and a clipped version of matrix representation. The influence of accidental blade equality on recognition is also studied. Finally, the efficiency of the GA model is compared to that of previously developed models.},
  archivePrefix = {arXiv},
  eprint = {1003.5899},
  eprinttype = {arxiv},
  file = {/home/ross/Zotero/storage/GJ58L4K8/Patyk-Lonska_2010_Geometric Algebra Model of Distributed Representations.pdf},
  keywords = {Artificial Intelligence,cogsci}
}

@phdthesis{plateDistributedRepresentationsNested1994,
  title = {Distributed {{Representations}} and {{Nested Compositional Structure}}},
  author = {Plate, Tony A.},
  year = {1994},
  abstract = {Distributed representations are attractive for a number of reasons. They offer the pos- sibility of representing concepts in a continuous space, they degrade gracefully with noise, and they can be processed in a parallel network of simple processing elements. However, the problem of representing nested structure in distributed representations has been for some time aprominent concern of both proponents and critics of connectionism [Fodor and Pylyshyn 1988; Smolensky 1990; Hinton 1990]. The lack of connectionist representations for complex structure has held back progress in tackling higher-level cognitive tasks such as language understanding and reasoning. In this thesis I review connectionist representations and propose a method for the distributed representation of nested structure, which I call ``Holographic Reduced Rep- resentations'' (HRRs). HRRs provide an implementation of Hinton's [1990] ``reduced descriptions''. HRRs use circular convolution to associate atomic items, which are repre- sented by vectors. Arbitrary variable bindings, short sequences of various lengths, and predicates can be represented in a fixed-width vector. These representations are items in their own right, and can be used in constructing compositional structures. The noisy re- constructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties. Circular convolution, which is the basic associative operator for HRRs, can be built into a recurrent neural network. The network can store and produce sequences. I show that neural network learning techniques can be used with circular convolution in order to learn representations for items and sequences. Oneof the attractions of connectionist representations of compositional structures is the possibility of computing without decomposing structures. I show that it is possible to use dot-product comparisons ofHRRsfor nested structures to estimate the analogical similarity of the structures. This demonstrates how the surface form of connectionist representations can reflect underlying structural similarity and alignment.},
  file = {/home/ross/Zotero/storage/DNMCK49C/Plate_1994_Distributed Representations and Nested Compositional Structure.pdf},
  keywords = {cogsci},
  school = {University of Toronto},
  type = {Doctor of {{Philosophy}}}
}

@inproceedings{plateHolographicReducedRepresentations1991,
  title = {Holographic {{Reduced Representations}}: {{Convolution Algebra}} for {{Compositional Distributed Representations}}},
  booktitle = {Proceedings of the 12th {{International Joint Conference}} on {{Artificial Intelligence}} ({{IJCAI}}'91)},
  author = {Plate, Tony A.},
  editor = {Mylopoulos, John and Reiter, Ray},
  year = {1991},
  pages = {30--35},
  publisher = {{Morgan Kaufmann}},
  address = {{San Mateo, CA, USA}},
  abstract = {A solution to the problem of representing compositional structure using distributed representations is described. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, frames, and reduced representations can be compressed into a fixed width vector. These representations are items in their own right, and can be used in constructing compositional structures. The noisy reconstructions given by convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties.},
  file = {/home/ross/Zotero/storage/4FKUFFM4/Plate_1991_Holographic Reduced Representations.pdf}
}

@inproceedings{plateRepresentingStructuredRelational2004,
  title = {Representing Structured Relational Data in {{Euclidean}} Vector Spaces},
  booktitle = {{{AAAI Fall Symposium Series}} 2004, {{Symposium}}: {{Compositional Connectionism}} in {{Cognitive Science}}},
  author = {Plate, Tony A.},
  year = {2004},
  file = {/home/ross/Zotero/storage/65KD3P3Z/Plate_2004_Representing structured relational data in Euclidean vector spaces.pdf}
}

@article{schlegelComparisonVectorSymbolic2020,
  title = {A Comparison of {{Vector Symbolic Architectures}}},
  author = {Schlegel, Kenny and Neubert, Peer and Protzel, Peter},
  year = {2020},
  month = jan,
  abstract = {Vector Symbolic Architectures (VSAs) combine a high-dimensional vector space with a set of carefully designed operators in order to perform symbolic computations with large numerical vectors. Major goals are the exploitation of their representational power and ability to deal with fuzziness and ambiguity. Over the past years, VSAs have been applied to a broad range of tasks and several VSA implementations have been proposed. The available implementations differ in the underlying vector space (e.g., binary vectors or complex-valued vectors) and the particular implementations of the required VSA operators - with important ramifications for the properties of these architectures. For example, not every VSA is equally well suited to address each task, including complete incompatibility. In this paper, we give an overview of eight available VSA implementations and discuss their commonalities and differences in the underlying vector space, bundling, and binding/unbinding operations. We create a taxonomy of available binding/unbinding operations and show an important ramification for non self-inverse binding operation using an example from analogical reasoning. A main contribution is the experimental comparison of the available implementations regarding (1) the capacity of bundles, (2) the approximation quality of non-exact unbinding operations, and (3) the influence of combined binding and bundling operations on the query answering performance. We expect this systematization and comparison to be relevant for development and evaluation of new VSAs, but most importantly, to support the selection of an appropriate VSA for a particular task.},
  archivePrefix = {arXiv},
  eprint = {2001.11797},
  eprinttype = {arxiv},
  file = {/home/ross/Zotero/storage/ZRFFDT8T/Schlegel et al_2020_A comparison of Vector Symbolic Architectures.pdf;/home/ross/Zotero/storage/68H26U7Y/2001.html},
  journal = {arXiv:2001.11797 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@article{schonemannAlgebraicRelationsInvolutions1987,
  title = {Some Algebraic Relations between Involutions, Convolutions, and Correlations, with Applications to Holographic Memories},
  author = {Sch{\"o}nemann, P. H.},
  year = {1987},
  month = jul,
  volume = {56},
  pages = {367--374},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00319516},
  abstract = {Convolutions * and correlations \# in spacesH of doubly infinite sequences are related bya\#b=S(a * Sb), whereS is an involution which reflects the order in the integral domainZ on which the sequences are defined. This relation can be used to represent a non-associative correlation algebra {$\langle$}H, \#{$\rangle$} by an associative convolution algebra equipped with the involutionS which, as is shown, greatly simplifies derivations. Related matrix representations of \#, *,S are given for sequences with finite support in Re n . Some implications for holographic memory models are discussed.},
  file = {/home/ross/Zotero/storage/SBYXZNKY/Sch√∂nemann_1987_Some algebraic relations between involutions, convolutions, and correlations,.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {5-6}
}

@article{smolenskyTensorProductVariable1990,
  title = {Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
  author = {Smolensky, Paul},
  year = {1990},
  month = nov,
  volume = {46},
  pages = {159--216},
  issn = {00043702},
  doi = {10.1016/0004-3702(90)90007-M},
  abstract = {A general method, the tensor product representation, is defined for the connectionist representation of value/variable bindings. The technique is a formalization of the idea that a set of value/variable pairs can be represented by accumulating activity in a collection of units each of which computes the product of a feature of a variable and a feature of its value. The method allows the fully distributed representation of bindings and symbolic structures. Fully and partially localized special cases of the tensor product representation reduce to existing cases of connectionist representations of structured data. The representation rests on a principled analysis of structure; it saturates gracefully as larger structures are represented; it permits recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it permits values to also serve as variables; and it enables analysis of the interference of symbolic structures stored in associative memories. It has also served as the basis for working connectionist models of high-level cognitive tasks.},
  file = {/home/ross/Zotero/storage/T683YAA4/Smolensky_1990_Tensor product variable binding and the representation of symbolic structures.pdf},
  journal = {Artificial intelligence},
  keywords = {cogsci},
  number = {1-2}
}

@article{venkatasubramanianJohnsonLindenstraussTransformEmpirical2011,
  title = {The {{Johnson}}-{{Lindenstrauss Transform}}: {{An Empirical Study}}},
  author = {Venkatasubramanian, Suresh and Wang, Qiushi},
  year = {2011},
  pages = {164--173},
  file = {/home/ross/Zotero/storage/8YWNLPAM/Venkatasubramanian_Wang_2011_The Johnson-Lindenstrauss Transform.pdf}
}


